# -*- coding: utf-8 -*-
"""Final Safa Samaj.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QNePxffjY0ptLBMIBXdA23XokYkslqmP

Safa-Samaj "A Waste Management System"
"""

!pip install tensorflow-gpu

!nvidia-smi

from google.colab import drive
drive.mount('/content/drive')

#import library
import numpy as np
import cv2
from keras.callbacks import ModelCheckpoint,EarlyStopping, CSVLogger
from keras.layers import Conv2D, Flatten, MaxPooling2D,Dense,Dropout,SpatialDropout2D
from keras.models  import Sequential
from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img, array_to_img
import random,os,glob
import matplotlib.pyplot as plt

#dataset input
dir_path = '/content/drive/MyDrive/Final Images for Safa Samaj'
train_path = '/content/drive/MyDrive/Final Images for Safa Samaj/dataset-resized'
validation_path ='/content/drive/MyDrive/Final Images for Safa Samaj/validation'

#img_list = glob.glob(os.path.join(dir_path, '/.jpg'))
img_list_train = glob.glob(os.path.join(train_path, '/.jpg'))
img_list_valid = glob.glob(os.path.join(validation_path, '/.jpg'))

#len(img_list)
len(img_list_train)
#len(img_list_valid)

#image augumentation
train=ImageDataGenerator(horizontal_flip=True,
                         vertical_flip=True,
                         validation_split=0.1,
                         rotation_range= 45,
                         rescale=1./255,
                         shear_range = 0.1,
                         zoom_range = 0.1,
                         width_shift_range = 0.1,
                         height_shift_range = 0.1,)

validation=ImageDataGenerator(rescale=1/255,
                        validation_split=0.1)

train_generator=train.flow_from_directory(train_path,
                                          target_size=(300,300),
                                          batch_size=32,
                                          shuffle=True,
                                          class_mode='categorical',
                                          subset='training')

validation_generator=validation.flow_from_directory(validation_path,
                                        target_size=(300,300),
                                        batch_size=32,
                                        shuffle=False,
                                        class_mode='categorical',
                                        subset='validation')

labels = (train_generator.class_indices)
print(labels)

labels = dict((v,k) for k,v in labels.items())
print(labels)

for image_batch, label_batch in train_generator:
  break
image_batch.shape, label_batch.shape

#writing the labels files
print (train_generator.class_indices)

Labels = '\n'.join(sorted(train_generator.class_indices.keys()))

with open('labels.txt', 'w') as f:
  f.write(Labels)

#building the keras network
model=Sequential()
#Convolution blocks

model.add(Conv2D(32,(3,3), padding='same',input_shape=(300,300,3),activation='relu'))
model.add(MaxPooling2D(pool_size=2)) 
#model.add(SpatialDropout2D(0.5)) # No accuracy

model.add(Conv2D(64,(3,3), padding='same',activation='relu'))
model.add(MaxPooling2D(pool_size=2)) 
#model.add(SpatialDropout2D(0.5))

model.add(Conv2D(32,(3,3), padding='same',activation='relu'))
model.add(MaxPooling2D(pool_size=2)) 

#Classification layers
model.add(Flatten())

model.add(Dense(64,activation='relu'))
#model.add(SpatialDropout2D(0.5))
model.add(Dropout(0.2))
model.add(Dense(32,activation='relu'))

model.add(Dropout(0.2))
model.add(Dense(6,activation='softmax'))

# using the logger and callbacks to store the instants of the model
filepath="CheckPoints.h5"

checkpoint1 = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')

#es=EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)

log_csv= CSVLogger('my_logs.csv',separator=',',append=False)

callbacks_list = [checkpoint1,log_csv]

#summarizing our model
model.summary()

#compiling our model using categorical cross entropy-loss function and adam optimizer
model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['acc']) # RMS PROP - No accuracy

#training our model
history = model.fit_generator(train_generator,
                              epochs=100,
                              steps_per_epoch=728//32,
                              validation_data=validation_generator,
                              validation_steps=62//32,
                              workers = 4,
                              callbacks=callbacks_list) 
#41 epoch - 75% #73- 76.9%
#78 epoch - 80%

#saving our model
model.save('Trained_Model.h5')

np.save('my_history.npy',history.history)

train_loss, train_acc = model.evaluate_generator(train_generator,steps=16)
validation_loss,validation_acc=model.evaluate_generator(validation_generator,steps=16)
print('Train: %.3f, Validation: %.3f' %(train_acc,validation_acc))

# plotting the history data
# accuracy graph
acc = history.history['acc']
val_acc = history.history['val_acc']

loss = history.history['loss']
val_loss = history.history['val_loss']

# ______________ Graph 1 -------------------------

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

# ______________ Graph 2 -------------------------

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,max(plt.ylim())])
plt.title('Training and Validation Loss')
plt.show()

# testing our prediction
from keras.preprocessing import image

img_path = '/content/drive/MyDrive/Final Images for Safa Samaj/dataset-resized/metal/metal (112).jpg'

img = image.load_img(img_path, target_size=(300, 300))
img = image.img_to_array(img, dtype=np.uint8)
img=np.array(img)/255.0

plt.title("Loaded Image")
plt.axis('off')
plt.imshow(img.squeeze())

p=model.predict(img[np.newaxis, ...])

#print("Predicted shape",p.shape)
print("Maximum Probability: ",np.max(p[0], axis=-1))
predicted_class = labels[np.argmax(p[0], axis=-1)]
print("Classified:",predicted_class)

classes=[]
prob=[]
print("\n-------------------Individual Probability--------------------------------\n")

for i,j in enumerate (p[0],0):
    print(labels[i].upper(),':',round(j*100,2),'%')
    classes.append(labels[i])
    prob.append(round(j*100,2))
    
def plot_bar_x():
    # this is for plotting purpose
    index = np.arange(len(classes))
    plt.bar(index, prob)
    plt.xlabel('Labels', fontsize=12)
    plt.ylabel('Probability', fontsize=12)
    plt.xticks(index, classes, fontsize=12, rotation=20)
    plt.title('Probability for loaded image')
    plt.show()
plot_bar_x()